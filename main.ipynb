{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation Restoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kai Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Restoration Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you are building a software for transcribing speech to text. The speech transcription part works perfectly, but cannot transcribe punctuations. The task is to train a predictive model to ingest a sequence of text and add punctuation (period, comma or question mark) in the appropriate locations. This task is important for all downstream data processing jobs. <br /><br />\n",
    "\n",
    "#### Example input:\n",
    "`this is a string of text with no punctuation this is a new sentence`\n",
    "\n",
    "<br />\n",
    "\n",
    "#### Example output:\n",
    "`this is a string of text with no punctuation <period> this is a new sentence <period>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input is text with punctuations removed.\n",
    "- The training data is case insensitive.\n",
    "- The model only prdicts period, comma or question mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Punctuation restoration is performed on a batch of transcription.\n",
    "- Output includes punctuation without annotations.\n",
    "- ASR output does not contain broken sentence.\n",
    "- The model is only trained on outputting period, comma or question mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although sentence must adhere to specific grammar rules, one might try to build a rule-based system for punctuation restoration. However, there are three weaknesses if one wants to do this:\n",
    "1. A common problem is that most people don't follow grammar rules when speaking.\n",
    "2. Writing a rule-based system for punctuation restoration requires prior knowledge on specific grammar rules for a language.\n",
    "3. The approach of building a rule-based system for punctuation restoration for a specific language can not generalize to other languages.\n",
    "\n",
    "Therefore, the following solution solves the above problems by incorporating [bidirectional recurrent neural networks (BRNN)](https://deeplearning.cs.cmu.edu/S20/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf) with an [attention mechanism](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) for punctuation restoration in unsegmented text.\n",
    "\n",
    "My solution is largely based on [Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF).\n",
    "\n",
    "The architecture is defined as follows:\n",
    "1. Obtain words embeddings from [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "2. The word embeddings are then processed by densely connected [Bi-LSTM](https://arxiv.org/pdf/1303.5778.pdf) layers.\n",
    "3. These Bi-LSTM layers are followed by a RNN with an attention mechanism and [conditional random field (CRF)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers) log likelihood loss.\n",
    "\n",
    "The reasons are as follows:\n",
    "1. GloVe obtains vector representations for words. It enforces the word vectors to capture sub-linear relationships in the vector space.\n",
    "2. BRNN enables the model to make use of unfixed length contexts before and after the current position in text. In the recurrent layers, I use [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf) which is well suited for capturing long range dependencies on multiple time scales.\n",
    "3. An attention mechanism further increases the model's capacity of finding relevant parts of the context for punctuation decisions. For example, the model might focus on words that indicate a question, but may be relatively far from the current word. An attention mechanism could nudge the model towards ending the sentence with a question mark instead of a period.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Settings, Libraries, Configurations and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from models.punctuation_restoration_model import PunctuationRestorationModel\n",
    "from utils.data_utils import split_to_batches\n",
    "from utils.data_utils import inhour\n",
    "from utils.punctuation_preprocess import process_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "tf.flags.DEFINE_string(\"raw_path\", \"data/raw/LREC_converted\", \"path to raw dataset\")\n",
    "tf.flags.DEFINE_string(\"save_path\", \"data/dataset/lrec\", \"path to save dataset\")\n",
    "tf.flags.DEFINE_string(\"glove_name\", \"840B\", \"glove embedding name\")\n",
    "\n",
    "# glove word embedding\n",
    "tf.flags.DEFINE_string(\"glove_path\", \"data/external/embeddings/glove.{}.{}d.txt\", \"glove embedding path\")\n",
    "tf.flags.DEFINE_integer(\"max_vocab_size\", 50000, \"maximal vocabulary size\")\n",
    "tf.flags.DEFINE_integer(\"max_sequence_len\", 200, \"maximal sequence length allowed\")\n",
    "tf.flags.DEFINE_integer(\"min_word_count\", 1, \"minimal word count in word vocabulary\")\n",
    "\n",
    "# dataset for train, validation and test\n",
    "tf.flags.DEFINE_string(\"vocab\", \"data/dataset/lrec/vocab.json\", \"path to the word and tag vocabularies\")\n",
    "tf.flags.DEFINE_string(\"train_word_counter\", \"data/dataset/lrec/train_word_counter.json\", \"path to the word counter \"\n",
    "                                                                                          \"in training datasets\")\n",
    "tf.flags.DEFINE_string(\"train_punct_counter\", \"data/dataset/lrec/train_punct_counter.json\", \"path to the punctuation \"\n",
    "                                                                                            \"counter in traning \"\n",
    "                                                                                            \"datasets\")\n",
    "tf.flags.DEFINE_string(\"dev_word_counter\", \"data/dataset/lrec/dev_word_counter.json\", \"path to the word counter in \"\n",
    "                                                                                      \"development datasets\")\n",
    "tf.flags.DEFINE_string(\"dev_punct_counter\", \"data/dataset/lrec/dev_punct_counter.json\", \"path to the punctuation \"\n",
    "                                                                                        \"counter in development \"\n",
    "                                                                                        \"datasets\")\n",
    "tf.flags.DEFINE_string(\"ref_word_counter\", \"data/dataset/lrec/ref_word_counter.json\", \"path to the word counter in \"\n",
    "                                                                                      \"ref test datasets\")\n",
    "tf.flags.DEFINE_string(\"ref_punct_counter\", \"data/dataset/lrec/ref_punct_counter.json\", \"path to the punctuation \"\n",
    "                                                                                        \"counter in ref test datasets\")\n",
    "tf.flags.DEFINE_string(\"asr_word_counter\", \"data/dataset/lrec/asr_word_counter.json\", \"path to the word counter in \"\n",
    "                                                                                      \"asr test datasets\")\n",
    "tf.flags.DEFINE_string(\"asr_punct_counter\", \"data/dataset/lrec/asr_punct_counter.json\", \"path to the punctuation \"\n",
    "                                                                                        \"counter in asr test datasets\")\n",
    "tf.flags.DEFINE_string(\"train_set\", \"data/dataset/lrec/train.json\", \"path to the training datasets\")\n",
    "tf.flags.DEFINE_string(\"dev_set\", \"data/dataset/lrec/dev.json\", \"path to the development datasets\")\n",
    "tf.flags.DEFINE_string(\"dev_text\", \"data/raw/LREC_converted/dev.txt\", \"path to the development text\")\n",
    "tf.flags.DEFINE_string(\"ref_set\", \"data/dataset/lrec/ref.json\", \"path to the ref test datasets\")\n",
    "tf.flags.DEFINE_string(\"ref_text\", \"data/raw/LREC_converted/ref.txt\", \"path to the ref text\")\n",
    "tf.flags.DEFINE_string(\"asr_set\", \"data/dataset/lrec/asr.json\", \"path to the asr test datasets\")\n",
    "tf.flags.DEFINE_string(\"asr_text\", \"data/raw/LREC_converted/asr.txt\", \"path to the asr text\")\n",
    "tf.flags.DEFINE_string(\"pretrained_emb\", \"data/dataset/lrec/glove_emb.npz\", \"pretrained embeddings\")\n",
    "\n",
    "# network parameters\n",
    "tf.flags.DEFINE_string(\"cell_type\", \"lstm\", \"RNN cell for encoder and decoder: [lstm | gru], default: lstm\")\n",
    "tf.flags.DEFINE_integer(\"num_layers\", 4, \"number of rnn layers\")\n",
    "tf.flags.DEFINE_multi_integer(\"num_units_list\", [50, 50, 50, 300], \"number of units for each rnn layer\")\n",
    "tf.flags.DEFINE_boolean(\"use_pretrained\", True, \"use pretrained word embedding\")\n",
    "tf.flags.DEFINE_boolean(\"tuning_emb\", False, \"tune pretrained word embedding while training\")\n",
    "tf.flags.DEFINE_integer(\"emb_dim\", 300, \"embedding dimension for encoder and decoder input words/tokens\")\n",
    "tf.flags.DEFINE_boolean(\"use_highway\", True, \"use highway network\")\n",
    "tf.flags.DEFINE_integer(\"highway_layers\", 2, \"number of layers for highway network\")\n",
    "tf.flags.DEFINE_boolean(\"use_crf\", True, \"use CRF decoder\")\n",
    "\n",
    "# training parameters\n",
    "tf.flags.DEFINE_float(\"lr\", 0.001, \"learning rate\")\n",
    "tf.flags.DEFINE_string(\"optimizer\", \"adam\", \"optimizer: [adagrad | sgd | rmsprop | adadelta | adam], default: adam\")\n",
    "tf.flags.DEFINE_boolean(\"use_lr_decay\", True, \"apply learning rate decay for each epoch\")\n",
    "tf.flags.DEFINE_float(\"lr_decay\", 0.05, \"learning rate decay factor\")\n",
    "tf.flags.DEFINE_float(\"l2_reg\", None, \"L2 norm regularization\")\n",
    "tf.flags.DEFINE_float(\"minimal_lr\", 1e-5, \"minimal learning rate\")\n",
    "tf.flags.DEFINE_float(\"grad_clip\", 2.0, \"maximal gradient norm\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\", 0.75, \"dropout keep probability for embedding while training\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"batch size\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 5, \"train epochs\")\n",
    "tf.flags.DEFINE_integer(\"max_to_keep\", 3, \"maximum trained models to be saved\")\n",
    "tf.flags.DEFINE_integer(\"no_imprv_tolerance\", 10, \"no improvement tolerance\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_path\", \"ckpt/punctuator/\", \"path to save models checkpoints\")\n",
    "tf.flags.DEFINE_string(\"summary_path\", \"ckpt/punctuator/summary/\", \"path to save summaries\")\n",
    "tf.flags.DEFINE_string(\"model_name\", \"punctuation_restoration_model\", \"models name\")\n",
    "\n",
    "# convert parameters to dict\n",
    "config = tf.flags.FLAGS.flag_values_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I test the model on the IWSLT datasets, which were originally used to evaluate ASR or SLT output. IWSLT datasets consist of TED talks which are openly available online. I used the same training, development and test set to train and test the model as the models published in [Punctuation Prediction for Unsegmented Transcript Based on Word Vector](https://hpi.de/fileadmin/user_upload/fachgebiete/meinel/papers/2016_Che_LREC.pdf) and [Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF). This helps in providing accuracy comparison on the model in a later section. The training and development set come from the IWSLT2012 machine trainslation track training data. IWSLT2011 punctuated reference transcripts and unpunctuated but segmented ASR transcripts are used for testing.  The detail about each dataset can be found in the Exploratory Data Analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preprocessing, I did the following steps:\n",
    "1. I follow the same procedure as [Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF) to deal with other punctuation symbols. They are either mapped to one of the punctuations in the output vocabulary or removed from corpora. To be more specific, exclamation marks and semicolons are mapped to periods, while colons and dashes are mapped to commas.\n",
    "2. I build vocabulary for each word that occurrs in the train set.\n",
    "3. I chunk the text into smaller observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data from raw data files\n",
    "if not os.path.exists(config[\"save_path\"]) or not os.listdir(config[\"save_path\"]):\n",
    "    process_data(config)\n",
    "if not os.path.exists(config[\"pretrained_emb\"]) and config[\"use_pretrained\"]:\n",
    "    process_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for training\n",
    "train_set = split_to_batches(config[\"train_set\"], config[\"batch_size\"], shuffle=True)\n",
    "# used for computing validate loss\n",
    "valid_data = split_to_batches(config[\"dev_set\"], config[\"batch_size\"], shuffle=True)[0]\n",
    "valid_text = config[\"dev_text\"]\n",
    "# used for evaluation metrics\n",
    "test_texts = [config[\"ref_text\"], config[\"asr_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_words_counter = load_dataset('data/dataset/lrec/train_word_counter.json')\n",
    "dev_words_counter = load_dataset('data/dataset/lrec/dev_word_counter.json')\n",
    "ref_words_counter = load_dataset('data/dataset/lrec/ref_word_counter.json')\n",
    "asr_words_counter = load_dataset('data/dataset/lrec/asr_word_counter.json')\n",
    "\n",
    "words_counter = [{'Dataset': 'Train', 'Total Number of Words': sum(train_words_counter.values()), 'Number of Unique Words': len(train_words_counter)},\n",
    "                 {'Dataset': 'Dev', 'Total Number of Words': sum(dev_words_counter.values()), 'Number of Unique Words': len(dev_words_counter)},\n",
    "                 {'Dataset': 'Ref', 'Total Number of Words': sum(ref_words_counter.values()), 'Number of Unique Words': len(ref_words_counter)},\n",
    "                 {'Dataset': 'ASR', 'Total Number of Words': sum(asr_words_counter.values()), 'Number of Unique Words': len(asr_words_counter)}]\n",
    "\n",
    "word_df = pd.DataFrame(words_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_punct_counter = load_dataset('data/dataset/lrec/train_punct_counter.json')\n",
    "dev_punct_counter = load_dataset('data/dataset/lrec/dev_punct_counter.json')\n",
    "ref_punct_counter = load_dataset('data/dataset/lrec/ref_punct_counter.json')\n",
    "asr_punct_counter = load_dataset('data/dataset/lrec/asr_punct_counter.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_df = pd.concat([pd.DataFrame([train_punct_counter]),pd.DataFrame([dev_punct_counter]),pd.DataFrame([ref_punct_counter]),pd.DataFrame([asr_punct_counter])],ignore_index=True)\n",
    "punct_df.columns = ['Number of Comma', 'Number of Period', 'Number of Question Mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Total Number of Words</th>\n",
       "      <th>Number of Unique Words</th>\n",
       "      <th>Number of Comma</th>\n",
       "      <th>Number of Period</th>\n",
       "      <th>Number of Question Mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>2089286</td>\n",
       "      <td>44514</td>\n",
       "      <td>158369</td>\n",
       "      <td>132330</td>\n",
       "      <td>9901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dev</td>\n",
       "      <td>293677</td>\n",
       "      <td>16150</td>\n",
       "      <td>22449</td>\n",
       "      <td>18910</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ref</td>\n",
       "      <td>12539</td>\n",
       "      <td>2317</td>\n",
       "      <td>830</td>\n",
       "      <td>806</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASR</td>\n",
       "      <td>12822</td>\n",
       "      <td>2317</td>\n",
       "      <td>798</td>\n",
       "      <td>809</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  Total Number of Words  Number of Unique Words  Number of Comma  \\\n",
       "0   Train                2089286                   44514           158369   \n",
       "1     Dev                 293677                   16150            22449   \n",
       "2     Ref                  12539                    2317              830   \n",
       "3     ASR                  12822                    2317              798   \n",
       "\n",
       "   Number of Period  Number of Question Mark  \n",
       "0            132330                     9901  \n",
       "1             18910                     1515  \n",
       "2               806                       46  \n",
       "3               809                       35  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_punct_df = pd.concat([word_df, punct_df], axis=1)\n",
    "words_punct_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "- Train set contains the most data.\n",
    "- Both test sets (Ref and ASR) contain similar data.\n",
    "- It is reasonalble that commas are used the most frequently when people talk. Question marks are used the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameters tuning, I could use [NNI (Neural Network Intelligence)](https://github.com/microsoft/nni) to tune hyperparameters (shown in Configurations section) in an efficient and automatic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word embedding shape: [None, None, 300]\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5365fd710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5365fd710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5365fdb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5365fdb90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae550>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae410>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fd5368ae590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd5368ae390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536891710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536891710>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fd590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fd590>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fdc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fdc90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fd610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5365fd610>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5368ae390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5368ae390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536870e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536870e90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5367a5790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd5367a5790>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536735310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd536735310>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "densely connected bi_rnn output shape: [None, None, 600]\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd53d21f690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd53d21f690>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534a903d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534a903d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534c6fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534c6fd90>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd535142a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fd535142a10>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534a90490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd534a90490>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd535bb8d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd535bb8d10>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd535b43490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd535b43490>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "attention output shape: [None, None, 300]\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd536807b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd536807b50>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "logits shape: [None, None, 4]\n",
      "params number: 5049620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch 1/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 498s - Global Step: 349 - Train Loss: 62.9904 - Perplexity: 35.3169   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on data/raw/LREC_converted/ref.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          56.21     38.19     45.48    \n",
      "<period>         70.67     61.37     65.69    \n",
      "<questionmark>   47.62     21.74     29.85    \n",
      "----------------------------------------------\n",
      "Overall          63.94     48.84     55.38    \n",
      "ERR: 8.14%\n",
      "SER: 61.2%\n",
      "\n",
      "\n",
      "Evaluate on data/raw/LREC_converted/asr.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          48.11     33.58     39.56    \n",
      "<period>         67.63     57.67     62.26    \n",
      "<questionmark>   52.63     28.57     37.04    \n",
      "----------------------------------------------\n",
      "Overall          58.81     45.34     51.20    \n",
      "ERR: 9.01%\n",
      "SER: 70.4%\n",
      "\n",
      " -- new BEST score on ref dataset: 55.38, on asr dataset: 51.20\n",
      "Epoch 2/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 502s - Global Step: 698 - Train Loss: 42.8125 - Perplexity: 4.0836   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on data/raw/LREC_converted/ref.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          61.63     45.66     52.46    \n",
      "<period>         71.09     73.91     72.47    \n",
      "<questionmark>   66.67     60.87     63.64    \n",
      "----------------------------------------------\n",
      "Overall          67.07     59.61     63.12    \n",
      "ERR: 7.0%\n",
      "SER: 52.6%\n",
      "\n",
      "\n",
      "Evaluate on data/raw/LREC_converted/asr.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          51.45     42.36     46.46    \n",
      "<period>         66.46     66.71     66.58    \n",
      "<questionmark>   50.00     40.00     44.44    \n",
      "----------------------------------------------\n",
      "Overall          59.56     54.30     56.81    \n",
      "ERR: 8.41%\n",
      "SER: 65.8%\n",
      "\n",
      " -- new BEST score on ref dataset: 63.12, on asr dataset: 56.81\n",
      "Epoch 3/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 501s - Global Step: 1047 - Train Loss: 37.2388 - Perplexity: 3.3253   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on data/raw/LREC_converted/ref.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          60.41     56.27     58.27    \n",
      "<period>         74.00     71.06     72.50    \n",
      "<questionmark>   77.14     58.70     66.67    \n",
      "----------------------------------------------\n",
      "Overall          67.43     63.41     65.36    \n",
      "ERR: 6.6%\n",
      "SER: 49.6%\n",
      "\n",
      "\n",
      "Evaluate on data/raw/LREC_converted/asr.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          48.84     50.25     49.54    \n",
      "<period>         68.10     64.48     66.24    \n",
      "<questionmark>   58.33     40.00     47.46    \n",
      "----------------------------------------------\n",
      "Overall          58.14     57.04     57.58    \n",
      "ERR: 8.45%\n",
      "SER: 66.1%\n",
      "\n",
      " -- new BEST score on ref dataset: 65.36, on asr dataset: 57.58\n",
      "Epoch 4/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 499s - Global Step: 1396 - Train Loss: 33.4364 - Perplexity: 2.9354   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on data/raw/LREC_converted/ref.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          60.89     57.59     59.20    \n",
      "<period>         73.83     72.55     73.18    \n",
      "<questionmark>   77.78     60.87     68.29    \n",
      "----------------------------------------------\n",
      "Overall          67.62     64.84     66.20    \n",
      "ERR: 6.46%\n",
      "SER: 48.6%\n",
      "\n",
      "\n",
      "Evaluate on data/raw/LREC_converted/asr.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          48.25     51.75     49.94    \n",
      "<period>         67.69     65.59     66.62    \n",
      "<questionmark>   62.96     48.57     54.84    \n",
      "----------------------------------------------\n",
      "Overall          57.62     58.50     58.06    \n",
      "ERR: 8.45%\n",
      "SER: 66.1%\n",
      "\n",
      " -- new BEST score on ref dataset: 66.20, on asr dataset: 58.06\n",
      "Epoch 5/5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 501s - Global Step: 1745 - Train Loss: 29.9849 - Perplexity: 2.6302   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on data/raw/LREC_converted/ref.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          61.80     57.11     59.36    \n",
      "<period>         72.69     72.42     72.56    \n",
      "<questionmark>   75.68     60.87     67.47    \n",
      "----------------------------------------------\n",
      "Overall          67.56     64.54     66.02    \n",
      "ERR: 6.53%\n",
      "SER: 49.1%\n",
      "\n",
      "\n",
      "Evaluate on data/raw/LREC_converted/asr.txt:\n",
      "----------------------------------------------\n",
      "PUNCTUATION      PRECISION RECALL    F-SCORE  \n",
      "<comma>          48.88     52.01     50.39    \n",
      "<period>         68.72     67.70     68.20    \n",
      "<questionmark>   56.67     48.57     52.31    \n",
      "----------------------------------------------\n",
      "Overall          58.45     59.66     59.05    \n",
      "ERR: 8.39%\n",
      "SER: 65.6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "model = PunctuationRestorationModel(config)\n",
    "\n",
    "# if the model is already trained, restore the model\n",
    "# model.restore_last_session()\n",
    "\n",
    "model.train(train_set, valid_data, valid_text, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "- From the Exploratory Data Analysis section, it is obvious that most words are not followed by any punctuation marks. The punctuation samples for the majority of these words can be succeessfully classified by the model and make the general accuracy (general accuracy = 1 - ERR) of the classification beyond 90%.\n",
    "- In pupose of predicting puncutation marks, we care more about the performance on predicting period, comma and question mark. Therefore, the words that are not followed by any punctuation marks are ignored. The model is evaluated in terms of per punctuation and overall precision, recall and F1 score. I also report the overall [slot error rate (SER)](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=46A91ADB72135D6B2D6589AA5A1D6AD3?doi=10.1.1.27.4637&rep=rep1&type=pdf), as F1 score has been shown to have some undesireable properties in [Performance Measures for Information Extraction](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=46A91ADB72135D6B2D6589AA5A1D6AD3?doi=10.1.1.27.4637&rep=rep1&type=pdf).\n",
    "- There are three types of errors: substitution, deletion and insertion.\n",
    "- Precision measures the percentage of correctly predicted punctuation marks in all predicted punctuation marks. It deals with substitution and insertion errors. Higher is better.\n",
    "- Recall measures the percentage of correctly predicted punctuation marks in all expected punctuation marks. It deals with substitution and deletion errors. Higher is better.\n",
    "- F1 score and SER are used to have a single measure of performance that deals with all three types of errors simultaneously. For F1 score, higher is better. In contrast, lower SER is better.\n",
    "- The model reaches the best performance on both reference transcripts and ASR output in 5 epochs.\n",
    "- The result shows that comma restoration is a harder task than period and question mark restoration. I think it is normal because the grammatical ambiguity of a pause is generally higher than a full-stop, espacially in less formal text like the transcript of TED talks.\n",
    "- The result shows that punctuation restoration is a harder task on ASR output than reference transcripts. I think it is normal because there are likely more noise and errors introduced in ASR output.\n",
    "- As we can see, the model achieves comparable performance comparing to the models published in [Punctuation Prediction for Unsegmented Transcript Based on Word Vector](https://hpi.de/fileadmin/user_upload/fachgebiete/meinel/papers/2016_Che_LREC.pdf) and [Bidirectional Recurrent Neural Network with Attention Mechanism for Punctuation Restoration](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Text 1:\n",
    "`this is a string of text with no punctuation this is a new sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a string of text with no punctuation <period> this is a new sentence <period>\n",
      "Inference Running Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "sentence = 'this is a string of text with no punctuation this is a new sentence'\n",
    "start_time = time.time()\n",
    "print(model.inference(sentence))\n",
    "print(\"Inference Running Time: {0}\".format(inhour(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Text 2:\n",
    "`hi this is wealthsimple customer service my name is john how can i help you today`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi <comma> this is wealthsimple customer service <period> my name is john <period> how can i help you today <questionmark>\n",
      "Inference Running Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "sentence = 'hi this is wealthsimple customer service my name is john how can i help you today'\n",
    "start_time = time.time()\n",
    "print(model.inference(sentence))\n",
    "print(\"Inference Running Time: {0}\".format(inhour(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Text 3:\n",
    "`our purpose is to make sure everyone has the ability to exercise that right it is great to wave a flag and talk about power to the people but how do you actually do it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our purpose is to make sure everyone has the ability to exercise that right <period> it is great to wave a flag and talk about power to the people <period> but how do you actually do it <questionmark>\n",
      "Inference Running Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "sentence = 'our purpose is to make sure everyone has the ability to exercise that right it is great to wave a flag and talk about power to the people but how do you actually do it'\n",
    "start_time = time.time()\n",
    "print(model.inference(sentence))\n",
    "print(\"Inference Running Time: {0}\".format(inhour(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Text 4:\n",
    "`account minimums are a barrier to entry that is why we do not have them that is why we make our fees as low as possible we use technology and force of will to bring down costs for our clients so they have control of more of their money`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account minimums are a barrier to entry <period> that is why we do not have them <period> that is why we make our fees as low as possible <period> we use technology and force of will to bring down costs for our clients <period> so they have control of more of their money <period>\n",
      "Inference Running Time: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "sentence = 'account minimums are a barrier to entry that is why we do not have them that is why we make our fees as low as possible we use technology and force of will to bring down costs for our clients so they have control of more of their money'\n",
    "start_time = time.time()\n",
    "print(model.inference(sentence))\n",
    "print(\"Inference Running Time: {0}\".format(inhour(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I would try to add and train a classification layer on [pre-trained BERT](https://github.com/huggingface/transformers) for punctuation restoration if I have more time for the following reasons:\n",
    "    - BERT uses transformers architecture of neural network whereas the current approach uses LSTM.\n",
    "    - Pre-trained BERT has more capacity and was trained a much larger corpus.\n",
    "2. I would like to have the model to take in more inputs such as pauses during conversations. In this way, the model can levarage more rich information in training and prediction.\n",
    "3. In the current approach, the word embeddings are fixed the whole time. I would try to allow the pretrained word embeddings to be tuned during training.\n",
    "4. According to [Character-Word LSTM Language Models](https://www.aclweb.org/anthology/E17-1040.pdf), character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. I would try to concatenate word and character embeddings  and\n",
    "feed the resulting character-word embedding to the LSTM in order to build a richer sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "restore_punct",
   "language": "python",
   "name": "restore_punct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
